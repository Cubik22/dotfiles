#!/bin/sh

info=$(cat << EOF
news-url-search [option...] url
search and get news feeds in url page
rss and atom

option:
    -h, --help
        print this help message
    -a, --all
        grep all occurances of rss and atom
    -s, --simple
        simply grep rss and atom without extracting urls
    -t, --try
        force try urls
        by default try urls just when no urls are found with grepping
EOF
)

all=false
simple=false
try=false

if [ "$#" -ne 0 ]; then
    while [ "$#" -ne 0 ]; do
        case "$1" in
            -h|--help)
                echo "$info"
                return 0
            ;;
            -a|--all)
                all=true
            ;;
            -s|--simple)
                simple=true
            ;;
            -t|--try)
                try=true
            ;;
            -*)
                echo "error: parameter $1 not recognized" >/dev/stderr
                echo "$info"
                return 1
            ;;
            *)
                break
            ;;
        esac
        shift 1
    done
else
    echo "error: insert url" >/dev/stderr
    echo "$info"
    return 1
fi

IFS='
'

url="$(
    echo "$1" |
    sed 's/\/\(#\|\)$//'
)"
last_sub="$(
    echo "$url" |
    rev |
    cut -d "/" -f 1 |
    rev
)"
case "$last_sub" in
    *.html|*.php)
        origin_url="$(
            echo "$url" |
            rev |
            cut -d "/" -f 2- |
            rev
        )"
    ;;
    *)
        origin_url="$url"
    ;;
esac

error_file="$(mktemp)"
website="$(curl --no-progress-meter --location "$url" 2>"$error_file")"
cat "$error_file" >/dev/stderr
rm -f "$error_file"

if [ "$all" = "false" ]; then
    target="application/\(rss\|atom\)+xml"
else
    target="rss\|atom"
fi

if [ "$simple" = "true" ]; then
    echo "$website" |
    grep --text --color=auto "$target"
    return 0
fi

search_link() {
    website="$1"
    target="$2"

    echo "$website" |
    grep --text "$target" |
    sed "s/^.*href=[\"|\']//" |
    sed "s/[\"|\'].*$//" |
    sed "s/\/\(#\|\)$//"
}

print_link() {
    links="$1"
    origin_url="$2"

    for link in $links; do
        case "$link" in
            /*|./*)
                link="$(echo "$link" | sed 's/^\.//')"
                echo "${origin_url}${link}"
            ;;
            ../*)
                two_dots="$(echo "$link" | cut -d "/" -f 1)"
                remaining_part="$(echo "$link" | cut -d "/" -f 2-)"
                origin_url_inside="$origin_url"
                while [ "$two_dots" = ".." ]; do
                    two_dots="$(echo "$remaining_part" | cut -d "/" -f 1)"
                    remaining_part="$(echo "$remaining_part" | cut -d "/" -f 2-)"
                    origin_url_inside="$(
                        echo "$origin_url_inside" |
                        rev |
                        cut -d "/" -f 2- |
                        rev
                    )"
                done
                echo "$origin_url_inside/$remaining_part"
            ;;
            *)
                echo "$link"
            ;;
        esac
    done
}

news_links="$(search_link "$website" "$target")"
[ -n "$news_links" ] && print_link "$news_links" "$origin_url"

if [ -n "$news_links" ] && [ "$try" = "false" ]; then
    return 0
fi

check_url_exist() {
    url_local="$1"

    if wget --spider "$url_local" 2>/dev/null; then
        return 0
    else
        return 1
    fi
}

subdomains_try="$(cat << EOF
index
index.xml
index.php
feed
feed.xml
feed.php
rss
rss.xml
rss.php
atom
atom.xml
atom.php
feed/rss
feed/atom
feeds/posts/default
feeds/news
EOF
)"
printf "\ntry urls:\n"
for sub_try in $subdomains_try; do
    try_url="$origin_url/$sub_try"
    check_url_exist "$try_url" && echo "$try_url"
done
